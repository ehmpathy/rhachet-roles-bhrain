# khue.3 answer: context window usage benchmarks for non-interactive claude invocation

## summary

yes, performance degrades near context limits. but the degradation is more nuanced than "close to limit = bad job":

1. **context rot** = performance degrades gradually as input tokens increase
2. **lost in the middle** = information in the middle 10-50% of context is retrieved worse than beginning/end
3. **autocompact** = claude code summarizes when ~95% full, losing granular detail but preserving key decisions

for non-interactive batch invocations, **optimal usage is 50-70% of context window** for best quality.

---

## the research

### context rot phenomenon

from [chroma research](https://research.trychroma.com/context-rot):
- performance consistently degrades as context length increases
- degradation is often non-uniform and surprising
- even "minimal conditions" show degradation with more input

from [aimultiple research](https://research.aimultiple.com/ai-context-window/):
- claude sonnet 4 (thinking) drops performance around 60k-120k tokens
- models claiming 200k typically become unreliable around 130k
- drops are often sudden, not gradual

### "lost in the middle" problem

from [snorkel.ai research](https://snorkel.ai/blog/long-context-models-in-the-enterprise-benchmarks-and-beyond/):
- u-shaped performance curve: models perform better on information at beginning/end
- accuracy degrades significantly for tokens at 10-50% depth
- 25% depth shows particularly poor recall (similar to gpt-4 findings)

from [databricks research](https://www.databricks.com/blog/long-context-rag-performance-llms):
- llama-3.1-405b performance drops after 32k tokens
- gpt-4-0125-preview drops after 64k tokens
- most models exhibit middle-degradation pattern

### claude-specific findings

from [claudelog](https://claudelog.com/mechanics/context-window-depletion/):
- "responses become generic, previous decisions forgotten, code quality degrades"
- "llms perform much worse when context window approaches limit"
- recommendation: avoid running claude to the limit

from [eval.16x.engineer](https://eval.16x.engineer/blog/llm-context-management-guide):
- claude claims <5% accuracy degradation across full window
- practical experience shows worse degradation in complex tasks

---

## autocompact mechanics

from [claudelog faq](https://claudelog.com/faqs/what-is-claude-code-auto-compact/) and [stevekinney.com](https://stevekinney.com/courses/ai-development/claude-code-compaction):

### when it triggers
- ~95% capacity (or 25% remaining)
- reserves ~40-45k tokens as buffer
- as of v2.0.64, compaction is instant

### what gets preserved
1. completed work (what tasks finished)
2. current state (files modified, status)
3. in progress (current work)
4. next steps (clear actions)
5. constraints (user preferences, decisions)
6. critical context

### what gets lost
- granular detail of earlier exchanges
- specific reasoning chains
- intermediate states and iterations
- exact error messages from earlier

---

## implications for the reviewer skill

### --hard mode (inject content)

**pros:**
- guarantees all rules + diffs + paths are in context
- no tool call overhead
- deterministic input (same content every run)

**cons:**
- risk of context rot if total tokens > 70% of window
- "lost in the middle" means rules in middle may be missed
- failfast at limit is good, but performance degrades before limit

**recommendations:**
- target 50-70% of context window max
- order matters: put most critical rules at START and END
- failfast threshold should be ~70%, not 95%
- emit token estimate AND warn when >60%

### --soft mode (let claude explore)

**pros:**
- agent retrieves only what's needed
- avoids context rot from unused content
- can handle larger rulesets by selective loading

**cons:**
- non-deterministic (different runs may load different context)
- tool call overhead increases latency
- may miss rules if glob patterns aren't explored fully

**recommendations:**
- good for large rulesets where not all apply
- still inject critical rules at prompt start
- use for exploratory reviews, not compliance checks

---

## benchmarks for batch/non-interactive invocation

no official anthropic benchmarks exist for "optimal batch usage". however, synthesizing research:

| context usage | expected quality | recommendation |
|---------------|------------------|----------------|
| 0-50% | excellent | safe zone |
| 50-70% | good | ideal for --hard mode |
| 70-85% | degrading | warn user, consider --soft |
| 85-95% | poor | failfast recommended |
| 95%+ | autocompact/fail | never reach this |

### practical thresholds for reviewer skill

```
const CONTEXT_THRESHOLDS = {
  SAFE: 0.50,        // green: proceed without warning
  WARN: 0.70,        // yellow: emit warning, suggest --soft
  FAILFAST: 0.85,    // red: fail with recommendation to split
};
```

---

## answer to the assumption

> assumption = if the input prompt is close to limit, claude will do a bad job? or will autocompact and loose info?

**both are true, but nuanced:**

1. **close to limit = degraded quality** (not necessarily "bad")
   - gradual degradation, not cliff-edge
   - middle content suffers most
   - complex multi-file tasks suffer more than single-focus tasks

2. **autocompact = lose granular detail** (not necessarily "lose info")
   - key decisions preserved
   - specific wording/reasoning lost
   - for review tasks: may lose nuance of why a rule was cited

3. **for non-interactive batch invocation:**
   - autocompact shouldn't trigger (single turn)
   - but if multi-turn review needed, compaction loses review context
   - better to fail early than produce degraded review

---

## sources

- [chroma research: context rot](https://research.trychroma.com/context-rot)
- [snorkel.ai: long context models](https://snorkel.ai/blog/long-context-models-in-the-enterprise-benchmarks-and-beyond/)
- [databricks: long context rag](https://www.databricks.com/blog/long-context-rag-performance-llms)
- [claudelog: context window depletion](https://claudelog.com/mechanics/context-window-depletion/)
- [claudelog: what is auto-compact](https://claudelog.com/faqs/what-is-claude-code-auto-compact/)
- [stevekinney: claude code compaction](https://stevekinney.com/courses/ai-development/claude-code-compaction)
- [hyperdev: how claude code got better](https://hyperdev.matsuoka.com/p/how-claude-code-got-better-by-protecting)
- [eval.16x.engineer: llm context management](https://eval.16x.engineer/blog/llm-context-management-guide)
- [aimultiple: ai context window](https://research.aimultiple.com/ai-context-window/)
