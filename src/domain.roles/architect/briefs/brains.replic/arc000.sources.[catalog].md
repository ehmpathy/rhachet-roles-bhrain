# research sources catalog

## .what

a comprehensive catalog of sources documenting replic brain architectures, reasoning patterns, and performance benchmarks.

## .why

these sources form the foundation for understanding how replic brains (LLMs behind REPLs) are constructed, compared, and evaluated.

---

## foundational papers

### reasoning patterns

| # | title | authors | url | date | relevance |
|---|-------|---------|-----|------|-----------|
| 1 | ReAct: Synergizing Reasoning and Acting in Language Models | Yao et al. | [arXiv:2210.03629](https://arxiv.org/abs/2210.03629) | 2022-10, ICLR 2023 | foundational pattern for interleaved reasoning + action in agentic loops |
| 2 | Chain-of-Thought Prompting Elicits Reasoning in Large Language Models | Wei et al. | [arXiv:2201.11903](https://arxiv.org/abs/2201.11903) | 2022-01, NeurIPS 2022 | foundational technique enabling step-by-step reasoning |
| 3 | Tree of Thoughts: Deliberate Problem Solving with Large Language Models | Yao et al. | [arXiv:2305.10601](https://arxiv.org/abs/2305.10601) | 2023-05, NeurIPS 2023 | exploration over reasoning paths, enables backtracking |
| 4 | Reflexion: Language Agents with Verbal Reinforcement Learning | Shinn et al. | [arXiv:2303.11366](https://arxiv.org/abs/2303.11366) | 2023-03 | self-reflection and iterative refinement for agents |
| 5 | Self-Consistency Improves Chain of Thought Reasoning | Wang et al. | [arXiv:2203.11171](https://arxiv.org/abs/2203.11171) | 2022-03 | multiple reasoning paths with majority voting |
| 6 | Language Agent Tree Search (LATS) | Zhou et al. | [arXiv:2310.04406](https://arxiv.org/abs/2310.04406) | 2023-10, ICML 2024 | Monte Carlo tree search for language agents |

### prompting techniques

| # | title | authors | url | date | relevance |
|---|-------|---------|-----|------|-----------|
| 7 | Plan-and-Solve Prompting | Wang et al. | [arXiv:2305.04091](https://arxiv.org/abs/2305.04091) | 2023-05 | zero-shot decomposition into planning + execution |
| 8 | Least-to-Most Prompting | Zhou et al. | [arXiv:2205.10625](https://arxiv.org/abs/2205.10625) | 2022-05 | decompose complex problems into simpler subproblems |
| 9 | Self-Ask: Measuring and Narrowing the Compositionality Gap | Press et al. | [arXiv:2210.03350](https://arxiv.org/abs/2210.03350) | 2022-10, ICLR 2023 | model asks follow-up questions before answering |
| 10 | Auto-CoT: Automatic Chain of Thought Prompting | Zhang et al. | [arXiv:2210.03493](https://arxiv.org/abs/2210.03493) | 2022-10 | automatic demonstration construction with diversity |
| 11 | Show Your Work: Scratchpads for Intermediate Computation | Nye et al. | [arXiv:2112.00114](https://arxiv.org/abs/2112.00114) | 2021-12 | intermediate computation steps improve multi-step tasks |

### tool use and function calling

| # | title | authors | url | date | relevance |
|---|-------|---------|-----|------|-----------|
| 12 | Toolformer: Language Models Can Teach Themselves to Use Tools | Schick et al. | [arXiv:2302.04761](https://arxiv.org/abs/2302.04761) | 2023-02 | self-supervised tool use learning |
| 13 | PAL: Program-Aided Language Models | Gao et al. | [arXiv:2211.10435](https://arxiv.org/abs/2211.10435) | 2022-11 | offload computation to program interpreter |

### embodied and agentic systems

| # | title | authors | url | date | relevance |
|---|-------|---------|-----|------|-----------|
| 14 | Inner Monologue: Embodied Reasoning through Planning with Language Models | Huang et al. | [arXiv:2207.05608](https://arxiv.org/abs/2207.05608) | 2022-07, CoRL | closed-loop feedback for robot planning |
| 15 | Generative Agents: Interactive Simulacra of Human Behavior | Park et al. | [arXiv:2304.03442](https://arxiv.org/abs/2304.03442) | 2023-04 | memory architecture for believable agent behavior |
| 16 | MemGPT: Towards LLMs as Operating Systems | Packer et al. | [arXiv:2310.08560](https://arxiv.org/abs/2310.08560) | 2023-10 | virtual context management, memory hierarchy |

### surveys and meta-analyses

| # | title | authors | url | date | relevance |
|---|-------|---------|-----|------|-----------|
| 17 | CoALA: Cognitive Architectures for Language Agents | Sumers et al. | [arXiv:2309.02427](https://arxiv.org/abs/2309.02427) | 2023-09 | systematic framework for language agent architectures |
| 18 | AgentBench: Evaluating LLMs as Agents | Liu et al. | [arXiv:2308.03688](https://arxiv.org/abs/2308.03688) | 2023-08 | comprehensive benchmark for evaluating LLM agents |
| 19 | Understanding the Planning of LLM Agents: A Survey | Huang et al. | [arXiv:2402.02716](https://arxiv.org/abs/2402.02716) | 2024-02 | taxonomy of planning approaches |
| 20 | ADaPT: As-Needed Decomposition and Planning | Prasad et al. | [arXiv:2311.05772](https://arxiv.org/abs/2311.05772) | 2023-11, NAACL 2024 | adaptive decomposition based on task complexity |
| 21 | Reasoning with Language Model Prompting: A Survey | Qiao et al. | [github.com/zjunlp](https://github.com/zjunlp/Prompt4ReasoningPapers) | 2023, ACL 2023 | comprehensive survey of prompting for reasoning |
| 22 | LLM-Based Agents for Tool Learning: A Survey | - | [Springer](https://link.springer.com/article/10.1007/s41019-025-00296-9) | 2024 | survey of tool learning approaches |
| 23 | A Survey of Task Planning with Large Language Models | - | [Intelligent Computing](https://spj.science.org/doi/10.34133/icomputing.0124) | 2024 | task planning and decomposition survey |

---

## claude-code architecture

| # | title | source | url | date | relevance |
|---|-------|--------|-----|------|-----------|
| 24 | Building Effective Agents | Anthropic blog | [anthropic.com/research](https://www.anthropic.com/research/building-effective-agents) | 2024-12 | official guidance on agent architecture patterns |
| 25 | Building Agents with the Claude Agent SDK | Anthropic engineering | [anthropic.com/engineering](https://www.anthropic.com/engineering/building-agents-with-the-claude-agent-sdk) | 2024 | SDK architecture and design principles |
| 26 | Advanced Tool Use on Claude | Anthropic engineering | [anthropic.com/engineering](https://www.anthropic.com/engineering/advanced-tool-use) | 2024 | tool search, programmatic calling, examples |
| 27 | Claude's Extended Thinking | Anthropic | [anthropic.com/news](https://www.anthropic.com/news/visible-extended-thinking) | 2025 | hybrid reasoning with thinking budgets |
| 28 | The "think" Tool: Enabling Claude to Stop and Think | Anthropic engineering | [anthropic.com/engineering](https://www.anthropic.com/engineering/claude-think-tool) | 2024 | structured thinking during tool use |
| 29 | Model Context Protocol (MCP) | Anthropic | [modelcontextprotocol.io](https://modelcontextprotocol.io/) | 2024 | standardized tool integration protocol |
| 30 | Claude Agent SDK Repository | GitHub | [github.com/anthropics/claude-agent-sdk-python](https://github.com/anthropics/claude-agent-sdk-python) | 2024 | reference implementation |
| 31 | Claude Code System Prompt Analysis | Zenn (community) | [zenn.dev](https://zenn.dev/) | 2024 | detailed analysis of system prompt structure |

---

## alternative architectures

### openai codex

| # | title | source | url | date | relevance |
|---|-------|--------|-----|------|-----------|
| 32 | Codex Cloud Architecture | OpenAI | [developers.openai.com/codex/cloud](https://developers.openai.com/codex/cloud/) | 2025 | cloud sandbox execution environment |
| 33 | Codex CLI Features | OpenAI | [developers.openai.com/codex/cli/features](https://developers.openai.com/codex/cli/features/) | 2025 | CLI capabilities and workflows |
| 34 | Codex Security Guide | OpenAI | [developers.openai.com/codex/security](https://developers.openai.com/codex/security/) | 2025 | sandboxing and network isolation |
| 35 | Codex SDK | OpenAI | [developers.openai.com/codex/sdk](https://developers.openai.com/codex/sdk/) | 2025 | programmatic integration |

### other coding assistants

| # | title | source | url | date | relevance |
|---|-------|--------|-----|------|-----------|
| 36 | Devin AI Architecture | various | (multiple sources) | 2024 | autonomous software engineer |
| 37 | Aider: AI Pair Programming | aider.chat | [aider.chat](https://aider.chat/) | 2024 | git-integrated coding assistant |
| 38 | Cursor AI | cursor.com | [cursor.com](https://cursor.com/) | 2024 | AI-augmented IDE |
| 39 | GitHub Copilot | GitHub | [github.blog](https://github.blog/) | 2024 | code completion and chat |

---

## context management

| # | title | source | url | date | relevance |
|---|-------|--------|-----|------|-----------|
| 40 | MemGPT: Virtual Context Management | Packer et al. | [arXiv:2310.08560](https://arxiv.org/abs/2310.08560) | 2023-10 | hierarchical memory for extended context |
| 41 | Extended Thinking Documentation | Anthropic | [support.claude.com](https://support.claude.com/en/articles/10574485-using-extended-thinking) | 2025 | thinking budgets and serial test-time compute |
| 42 | Context Window Management Strategies | various | (multiple sources) | 2024 | summarization and compaction techniques |

---

## benchmarks and performance

### code generation benchmarks

| # | title | source | url | date | relevance |
|---|-------|--------|-----|------|-----------|
| 43 | SWE-bench: Software Engineering Benchmark | Princeton NLP | [swebench.com](https://www.swebench.com/) | 2024 | real-world github issue resolution |
| 44 | HumanEval Benchmark | OpenAI | [paperswithcode.com](https://paperswithcode.com/dataset/humaneval) | 2021 | function synthesis from docstrings |
| 45 | HumanEval Pro and MBPP Pro | - | [arXiv:2412.21199](https://arxiv.org/abs/2412.21199) | 2024-12, ACL 2025 | self-invoking code generation |
| 46 | MBPP Benchmark | Google | [paperswithcode.com](https://paperswithcode.com/sota/code-generation-on-mbpp) | 2021 | mostly basic python problems |
| 47 | EvalPlus Leaderboard | - | [evalplus.github.io](https://evalplus.github.io/leaderboard.html) | 2024 | augmented test suites for HumanEval/MBPP |

### agent benchmarks

| # | title | source | url | date | relevance |
|---|-------|--------|-----|------|-----------|
| 48 | AgentBench | Liu et al. | [arXiv:2308.03688](https://arxiv.org/abs/2308.03688) | 2023-08 | multi-environment agent evaluation |
| 49 | LiveBench | - | [livebench.ai](https://livebench.ai/) | 2024 | continuously updated LLM evaluation |

### tool comparisons

| # | title | source | url | date | relevance |
|---|-------|--------|-----|------|-----------|
| 50 | Coding Agents Comparison | Artificial Analysis | [artificialanalysis.ai](https://artificialanalysis.ai/insights/coding-agents-comparison) | 2024 | Claude Code, Cursor, Copilot comparison |
| 51 | AI Coding Agents Benchmark 2025 | Render | [render.com/blog](https://render.com/blog/ai-coding-agents-benchmark) | 2025 | practical benchmark results |
| 52 | Best AI Code Apply Tools 2025 | Morph | [morphllm.com/comparisons](https://www.morphllm.com/comparisons) | 2025 | enterprise benchmarks |
| 53 | Claude Code vs Cursor Deep Comparison | Qodo | [qodo.ai/blog](https://www.qodo.ai/blog/claude-code-vs-cursor/) | 2025 | detailed feature comparison |

---

## reasoning strategy comparisons

| # | title | source | url | date | relevance |
|---|-------|--------|-----|------|-----------|
| 54 | ReAct vs CoT Performance | Google Research | [research.google/blog](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/) | 2022 | comparative analysis on HotPotQA, FEVER, ALFWorld |
| 55 | Comprehensive Guide to ReAct Prompting | Mercity | [mercity.ai/blog-post](https://www.mercity.ai/blog-post/react-prompting-and-react-based-agentic-systems) | 2024 | practical guide with performance notes |

---

## key performance findings

### reasoning strategies (from sources)

| strategy | benchmark | performance | source |
|----------|-----------|-------------|--------|
| ReAct | HotPotQA | competitive with CoT, better grounding | [1] |
| ReAct | ALFWorld | +34% over imitation learning | [1] |
| CoT | HotPotQA | higher success, but 56% hallucination in failures | [1] |
| Self-Consistency | various | significant improvement over single-path CoT | [5] |
| Tree of Thoughts | Game of 24 | 74% (vs 4% CoT) | [3] |
| LATS | HotPotQA | state-of-the-art with MCTS | [6] |
| Reflexion | ALFWorld | +22% improvement via self-reflection | [4] |

### code generation (from sources)

| model/tool | HumanEval | MBPP | SWE-bench | source |
|------------|-----------|------|-----------|--------|
| o1-mini | 96.2% | - | - | [45] |
| o1-mini (Pro) | 76.2% | - | - | [45] |
| GPT-4o + planning | - | 84.8% | - | [43] |
| Claude Code | - | - | ~49% (verified) | [44] |

---

## access date

all sources accessed: 2025-12-23
