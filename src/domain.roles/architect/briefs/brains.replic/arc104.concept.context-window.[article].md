# context-window

## .what

the fixed-size buffer of tokens that an llm can process in a single forward pass, representing its working memory during generation.

## .why

the context window is the fundamental constraint that shapes all replic brain architectures. it determines how much conversation history, code, and tool results can be processed simultaneously. strategies like context compaction, subagents, and memory hierarchies exist specifically to work within or around this limit.

## dependsOn

- `llm` — context window is a property of the llm

## key characteristics

- **fixed size**: measured in tokens (e.g., 200k tokens for claude)
- **attention-based**: all tokens attend to all other tokens (O(n²) complexity)
- **includes everything**: system prompt, conversation, tool calls, tool results
- **ephemeral**: cleared between sessions (no persistent memory)

## size examples (2024-2025)

| model | context window |
|-------|----------------|
| gpt-4o | 128k tokens |
| claude 3.5/4 | 200k tokens |
| gemini 1.5 | 1m+ tokens |

## implications for architecture

- longer context = more code/conversation in single pass
- summarization needed when context fills
- subagents can isolate context usage
- caching optimizes repeated prompts

## sources

- [MemGPT](https://arxiv.org/abs/2310.08560) — virtual context management
- [Claude Documentation](https://docs.anthropic.com) — 200k token context
