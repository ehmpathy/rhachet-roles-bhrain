# blueprint: implement replic brain documentation

## overview

this blueprint describes the step-by-step execution plan for fulfilling the wish.

the wish = produce briefs documenting replic brain architectures

the output = `src/roles/architect/briefs/brains.replic/*.md`

---

## phases

### phase.0: setup architect role

**operation**: `createArchitectRole`

**steps**:
1. create directory `src/roles/architect/`
2. create `src/roles/architect/readme.md` with content:
   ```md
   ## üèóÔ∏è Architect

   - **scale**: conceptual, architectural
   - **focus**: brain architecture patterns
   - **maximizes**: understanding of how artificial brains are constructed

   used to document and compare architectures of replic brains (LLMs behind REPLs).
   ```
3. create directory `src/roles/architect/briefs/brains.replic/`

**acceptance**: directories and readme exist

---

### phase.1: exhaustive literature search

**operation**: `searchLiterature`

**steps**:
1. search for claude-code architecture sources:
   - "claude code architecture agentic loop"
   - "anthropic claude agent SDK"
   - "claude code system prompt"
2. search for foundational academic papers:
   - "ReAct reasoning acting LLM paper"
   - "chain of thought prompting paper"
   - "LLM tool calling function calling"
   - "alternatives to ReAct agent pattern"
   - "reflexion agent self-reflection LLM"
   - "LATS language agent tree search"
   - "plan and solve prompting"
   - "self-ask prompting decomposition"
   - "toolformer LLM tool use paper"
   - "tree of thoughts deliberate problem solving"
   - "self-consistency chain of thought"
   - "least-to-most prompting decomposition"
   - "automatic chain of thought Auto-CoT"
   - "program-aided language models PAL"
   - "scratchpad reasoning LLM"
   - "inner monologue embodied reasoning"
   - "task decomposition LLM agents survey"
   - "LLM agents survey paper 2024"
   - "cognitive architectures for language agents"
   - "AgentBench benchmark LLM agents"
   - "SWE-bench software engineering benchmark"
   - "HumanEval code generation benchmark"
   - "MemGPT memory management LLM"
   - "generative agents simulacra paper"
3. search for alternative architectures:
   - "OpenAI Codex architecture 2025"
   - "Devin AI architecture autonomous coding"
   - "Aider coding assistant architecture"
   - "Cursor AI agent architecture"
4. search for context management:
   - "LLM context window management summarization"
   - "agent memory architecture"
5. search for performance benchmarks and comparisons:
   - "ReAct vs chain of thought performance comparison"
   - "tree of thoughts benchmark results"
   - "reflexion agent performance improvement"
   - "SWE-bench leaderboard results 2024 2025"
   - "HumanEval pass@k benchmark results"
   - "MBPP benchmark LLM code generation"
   - "AgentBench results comparison"
   - "reasoning strategy benchmark comparison LLM"
   - "agentic coding benchmark comparison"
   - "claude code vs cursor vs aider benchmark"
6. collect and deduplicate sources

**acceptance**: 50+ unique sources gathered with title, url, date, relevance

---

### phase.2: create research catalog brief

**operation**: `setBrief` [catalog]

**output path**: `src/roles/architect/briefs/brains.replic/arc000.sources.[catalog].md`

**steps**:
1. format all sources from phase.1 into catalog structure
2. organize by topic area:
   - foundational papers
   - claude-code specific
   - alternative architectures
   - context management
   - benchmarks and performance
3. include for each source:
   - title
   - authors (if known)
   - url
   - date accessed
   - relevance note

**acceptance**: catalog brief exists with all sources organized

---

### phase.3: distill atomic concepts

**operation**: `setConcept` √ó N

**output paths**: `src/roles/architect/briefs/brains.replic/arc1XX.concept.*.md`

**concepts to define** (discovered from research; example list below ‚Äî actual concepts TBD based on phase.1 findings):

| #   | concept              | depends on                             | brief                                                                    |
| --- | -------------------- | -------------------------------------- | ------------------------------------------------------------------------ |
| 101 | `llm`                | -                                      | the large language model that powers reasoning                           |
| 102 | `repl`               | -                                      | read-eval-print-loop pattern                                             |
| 103 | `replic-brain`       | llm, repl                              | LLM operating behind a REPL                                              |
| 104 | `context-window`     | llm                                    | working memory constraint                                                |
| 105 | `system-prompt`      | llm                                    | initial behavior instructions                                            |
| 106 | `tool-definition`    | -                                      | schema describing a tool's interface                                     |
| 107 | `tool-call`          | tool-definition                        | request to invoke a tool                                                 |
| 108 | `tool-result`        | tool-call                              | response from tool execution                                             |
| 109 | `agentic-loop`       | tool-call, tool-result, context-window | the core while(tool_call) pattern                                        |
| 110 | `reasoning-trace`    | llm                                    | chain-of-thought output                                                  |
| 111 | `react-pattern`      | reasoning-trace, agentic-loop          | interleaved reasoning + action (include benchmark performance)           |
| 112 | `reflexion-pattern`  | reasoning-trace, agentic-loop          | self-reflection and iterative refinement (include benchmark performance) |
| 113 | `tree-of-thoughts`   | reasoning-trace                        | deliberate search over thought branches (include benchmark performance)  |
| 114 | `context-compaction` | context-window                         | summarization to manage memory                                           |
| 115 | `subagent`           | agentic-loop                           | spawned child agent with isolated context                                |
| 116 | `session`            | agentic-loop, context-window           | single interaction instance                                              |
| 117 | `message`            | session                                | unit of conversation                                                     |

**steps**:
1. catalog distinct atomic concepts discovered from phase.1 research
2. identify dependency relationships between concepts
3. for each concept, create brief following [article] archetype:
   - include `.what` = clear definition
   - include `.why` = purpose in the architecture
   - include collocated sources
   - include benchmark performance data where applicable
   - declare `dependsOn` references

**acceptance**: each concept has its own [article] brief

---

### phase.4: create treestruct brief

**operation**: `buildConceptTreestruct`, then `setBrief` [article]

**output path**: `src/roles/architect/briefs/brains.replic/arc150.concepts.treestruct.[article].md`

**steps**:
1. compute dependency graph from concept.dependsOn
2. render as treestruct (illustrative example below ‚Äî actual structure TBD from phase.3):
   ```
   replic-brain
   ‚îú‚îÄ‚îÄ llm
   ‚îÇ   ‚îú‚îÄ‚îÄ context-window
   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ context-compaction
   ‚îÇ   ‚îú‚îÄ‚îÄ system-prompt
   ‚îÇ   ‚îî‚îÄ‚îÄ reasoning-trace
   ‚îÇ       ‚îú‚îÄ‚îÄ react-pattern
   ‚îÇ       ‚îú‚îÄ‚îÄ reflexion-pattern
   ‚îÇ       ‚îî‚îÄ‚îÄ tree-of-thoughts
   ‚îú‚îÄ‚îÄ repl
   ‚îî‚îÄ‚îÄ agentic-loop
       ‚îú‚îÄ‚îÄ tool-call
       ‚îÇ   ‚îî‚îÄ‚îÄ tool-definition
       ‚îú‚îÄ‚îÄ tool-result
       ‚îú‚îÄ‚îÄ session
       ‚îÇ   ‚îî‚îÄ‚îÄ message
       ‚îî‚îÄ‚îÄ subagent
   ```
3. include links to each concept's [article] brief

**acceptance**: treestruct brief shows composition hierarchy with navigable links

---

### phase.5: distill claude-code blueprint

**operation**: `setBlueprint`

**output path**: `src/roles/architect/briefs/brains.replic/arc201.blueprint.claude-code.[article].md`

**steps**:
1. document the core agentic loop:
   ```
   while true:
     response = llm.generate(context_window)
     if response.has_tool_calls:
       for tool_call in response.tool_calls:
         result = execute_tool(tool_call)
         context_window.append(result)
     else:
       return response  // loop terminates
   ```
2. document context management strategy:
   - auto-compaction via summarization
   - h2A queue for mid-task interjections
3. document tool interface:
   - 20+ builtin tools (Read, Write, Bash, etc.)
   - MCP extensibility
4. document subagent pattern:
   - Explore, Plan, Task subagent types
   - isolated context windows
5. include collocated sources for each claim

**acceptance**: blueprint brief contains pseudocode + all architecture details

---

### phase.6: distill alternative blueprint

**operation**: `setBlueprint`

**output path**: `src/roles/architect/briefs/brains.replic/arc202.blueprint.codex.[article].md`

**alternative**: OpenAI Codex (chosen because well-documented, comparable scope)

**steps**:
1. document the core agentic loop (cloud sandbox execution)
2. document context management (full context, long-running sessions)
3. document tool interface (tests, iteration, PR proposals)
4. document parallel task pattern
5. include collocated sources

**acceptance**: alternative blueprint follows same structure as claude-code

---

### phase.7: create comparison catalog

**operation**: `compareBlueprints`, then `setBrief` [catalog]

**output path**: `src/roles/architect/briefs/brains.replic/arc300.blueprints.comparison.[catalog].md`

**steps**:
1. define comparison dimensions:
   - loop architecture (single-threaded vs multi-agent)
   - context management (compaction vs full context)
   - tool interface (native vs MCP vs sandbox)
   - subagent pattern (typed vs parallel)
   - git integration (manual vs auto-commit)
2. define performance dimensions:
   - SWE-bench scores (if available)
   - HumanEval pass@k (if available)
   - task completion rates
   - reasoning strategy effectiveness (ReAct vs alternatives)
   - token efficiency
3. create comparison matrix (architecture + performance)
4. identify shared patterns across architectures
5. identify divergent approaches
6. synthesize insights on which patterns correlate with better performance

**acceptance**: comparison catalog maps similarities and differences

---

### phase.8: verify brief patterns

**operation**: verify against `src/roles/thinker/briefs/knowledge` patterns

**checklist**:
- [ ] [article] briefs define concepts clearly
- [ ] [catalog] briefs organize related items
- [ ] sources are collocated inline, not separate
- [ ] briefs follow naming convention: `arcNNN.topic.[archetype].md`
- [ ] each brief has `.what` and `.why` where appropriate

**acceptance**: all briefs conform to established patterns

---

## output manifest

upon completion, the following files will exist (concept list is illustrative ‚Äî actual concepts TBD from research):

```
src/roles/architect/
‚îú‚îÄ‚îÄ readme.md
‚îî‚îÄ‚îÄ briefs/
    ‚îî‚îÄ‚îÄ brains.replic/
        ‚îú‚îÄ‚îÄ arc000.sources.[catalog].md
        ‚îú‚îÄ‚îÄ arc1XX.concept.*.[article].md          # N concept briefs discovered from research
        ‚îú‚îÄ‚îÄ arc150.concepts.treestruct.[article].md
        ‚îú‚îÄ‚îÄ arc201.blueprint.claude-code.[article].md
        ‚îú‚îÄ‚îÄ arc202.blueprint.codex.[article].md
        ‚îî‚îÄ‚îÄ arc300.blueprints.comparison.[catalog].md
```

---

## execution notes

- phases can be executed sequentially by a single agent
- phase.3 (concepts) is the bulk of the work
- phases 5-6 (blueprints) require careful source citation
- idempotent: re-running any phase should produce consistent results
