# llm (large language model)

## .what

a neural network trained on vast text corpora that predicts the next token in a sequence, enabling it to generate coherent text, follow instructions, and perform reasoning.

## .why

the llm is the core intelligence of a replic brain. it provides the reasoning and generation capabilities that power all downstream behaviors — from understanding natural language to generating code to deciding which tools to invoke.

## dependsOn

- (none — foundational primitive)

## key characteristics

- **autoregressive generation**: produces output one token at a time, conditioning on all previous tokens
- **context window**: has a fixed maximum number of tokens it can process at once
- **emergent capabilities**: reasoning, instruction-following, and tool use emerge at scale
- **stochastic**: outputs are probabilistic, controlled via temperature parameter

## sources

- [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903) — demonstrates reasoning emergence in LLMs
- [CoALA: Cognitive Architectures for Language Agents](https://arxiv.org/abs/2309.02427) — positions LLM as central to agent architecture
