# context-compaction

## .what

techniques for reducing the token count of accumulated context while preserving essential information, enabling long-running agent sessions within fixed context window limits.

## .why

as replic brains iterate through the agentic loop, context accumulates: tool results, conversation turns, reasoning traces. without compaction, the context window fills and the agent cannot continue. compaction strategies allow sessions to persist indefinitely while retaining relevant information.

## dependsOn

- `context-window` — the constraint being managed
- `llm` — may perform summarization
- `agentic-loop` — produces context to compact

## strategies

### summarization
condense earlier conversation/results into summaries:
```
[Original: 5000 tokens of tool results]
       ↓ summarize
[Summary: 200 tokens capturing key findings]
```

### sliding window
keep only the most recent N turns:
```
[turn 1] [turn 2] [turn 3] [turn 4] [turn 5]
       ↓ window of 3
                 [turn 3] [turn 4] [turn 5]
```

### hierarchical memory (MemGPT)
tier information by recency/importance:
```
L1 (context): current working set
L2 (recall): summarized past sessions
L3 (archive): compressed long-term storage
```

### selective retention
keep only tool results that inform current task:
```
[file read A] [file read B] [file read C]
       ↓ task now focuses on C
                             [file read C]
```

## tradeoffs

| strategy | preserves | loses |
|----------|-----------|-------|
| summarization | gist | verbatim details |
| sliding window | recency | early context |
| hierarchical | structure | fast access to old data |
| selective | relevance | potentially useful info |

## implementation in replic brains

| system | compaction approach |
|--------|---------------------|
| claude code | auto-summarization when context fills |
| codex cloud | full context (large windows) |
| aider | git diff-based context |

## sources

- [MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560) — hierarchical memory
- [Building Effective Agents](https://www.anthropic.com/research/building-effective-agents) — summarization patterns
